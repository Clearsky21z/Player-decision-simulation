{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Player Embedding Training\n",
    "\n",
    "Train a SoccerMap model with learnable player embeddings.  \n",
    "After training, extract embeddings for player-similarity analysis.\n",
    "\n",
    "**Pipeline:**\n",
    "1. Load multi-match data (Leverkusen 2023/24 Bundesliga)\n",
    "2. Build global player-ID mapping\n",
    "3. Expand events with actor identity\n",
    "4. Create `PassDatasetWithIdentity` per match\n",
    "5. Train pass-selection model\n",
    "6. Evaluate & visualize\n",
    "7. Player similarity analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec1-header",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": "import sys, os\nfrom pathlib import Path\n\nsys.dont_write_bytecode = True\nos.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"  # prevent OMP libiomp5md.dll crash\n\n# Find project root\np = Path.cwd()\nfor _ in range(10):\n    if (p / \"environment.yml\").exists() or (p / \"README.md\").exists():\n        PROJECT_ROOT = p\n        break\n    p = p.parent\nelse:\n    raise RuntimeError(\"Cannot find project root.\")\n\nCODE_DIR = PROJECT_ROOT / \"code\"\nTED_DIR = PROJECT_ROOT / \"notebooks\" / \"sandbox\" / \"ted\"\n\nfor d in [str(CODE_DIR), str(TED_DIR)]:\n    if d not in sys.path:\n        sys.path.insert(0, d)\n\nDATA_ROOT = PROJECT_ROOT / \"data\" / \"leverkusen_data\"\n\nprint(\"PROJECT_ROOT:\", PROJECT_ROOT)\nprint(\"CODE_DIR:    \", CODE_DIR)\nprint(\"DATA_ROOT:   \", DATA_ROOT)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "\n",
    "from player_embedding import (\n",
    "    load_match_data,\n",
    "    build_events_dataframe,\n",
    "    load_lineup_dataframe,\n",
    "    find_matches_with_360,\n",
    "    find_seasons_for_competition,\n",
    "    load_team_matches,\n",
    "    build_player_id_mapping,\n",
    "    build_expanded_dfs_with_identity,\n",
    "    PassDatasetWithIdentity,\n",
    "    SoccerMapWithEmbedding,\n",
    "    pass_selection_loss,\n",
    "    pass_success_loss,\n",
    "    pass_selection_surface,\n",
    "    pass_success_surface,\n",
    "    GridSpec,\n",
    ")\n",
    "\n",
    "print(\"All imports OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec2-header",
   "metadata": {},
   "source": [
    "## 2. Discover matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "discover-matches",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEAM_NAME = \"Bayer Leverkusen\"\n",
    "COMPETITION_ID = 9  # Bundesliga\n",
    "\n",
    "seasons = find_seasons_for_competition(COMPETITION_ID, base_dir=str(DATA_ROOT))\n",
    "# pick the season with 360 data\n",
    "cand = [s for s in seasons if s.get(\"match_available_360\") not in [None, False, \"None\"]]\n",
    "SEASON_ID = cand[0][\"season_id\"] if cand else seasons[0][\"season_id\"]\n",
    "print(f\"Season: {SEASON_ID}\")\n",
    "\n",
    "matches = load_team_matches(\n",
    "    team_name=TEAM_NAME,\n",
    "    competition_id=COMPETITION_ID,\n",
    "    season_id=SEASON_ID,\n",
    "    base_dir=str(DATA_ROOT),\n",
    ")\n",
    "match_ids_360 = find_matches_with_360(matches, base_dir=str(DATA_ROOT))\n",
    "print(f\"Matches with 360 data: {len(match_ids_360)}\")\n",
    "print(f\"First 5: {match_ids_360[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec3-header",
   "metadata": {},
   "source": [
    "## 3. Load data & build global player mapping\n",
    "\n",
    "We load all matches to build a single player-ID mapping,  \n",
    "then hold out the last match for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many matches to use (set to len(match_ids_360) for full training)\n",
    "MAX_MATCHES = 5  # start small for quick iteration\n",
    "HOLDOUT = 1       # number of matches reserved for validation\n",
    "\n",
    "selected_ids = match_ids_360[:MAX_MATCHES]\n",
    "train_ids = selected_ids[:-HOLDOUT]\n",
    "val_ids = selected_ids[-HOLDOUT:]\n",
    "\n",
    "print(f\"Train matches: {len(train_ids)} -> {train_ids}\")\n",
    "print(f\"Val matches:   {len(val_ids)}   -> {val_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-all",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load combined events + lineups for ALL selected matches\n",
    "all_combined = {}   # match_id -> combined list\n",
    "all_lineups = {}    # match_id -> lineup_df\n",
    "all_events_df = {}  # match_id -> events_df\n",
    "\n",
    "for mid in selected_ids:\n",
    "    combined = load_match_data(mid, base_dir=str(DATA_ROOT))\n",
    "    lineup_df = load_lineup_dataframe(mid, base_dir=str(DATA_ROOT))\n",
    "    events_df = build_events_dataframe(combined)\n",
    "    all_combined[mid] = combined\n",
    "    all_lineups[mid] = lineup_df\n",
    "    all_events_df[mid] = events_df\n",
    "    print(f\"  match {mid}: {len(combined)} events, {len(lineup_df)} players in lineup\")\n",
    "\n",
    "print(f\"\\nLoaded {len(selected_ids)} matches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-mapping",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build global mapping across ALL matches (train + val)\n",
    "merged_events = pd.concat(all_events_df.values(), ignore_index=True)\n",
    "merged_lineups = pd.concat(all_lineups.values(), ignore_index=True)\n",
    "\n",
    "player_mapping = build_player_id_mapping(merged_events, merged_lineups)\n",
    "num_players = len(player_mapping)\n",
    "\n",
    "print(f\"Global player mapping: {num_players} players (indices 1..{num_players}, 0=padding)\")\n",
    "print(f\"Sample: {dict(list(player_mapping.items())[:5])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec4-header",
   "metadata": {},
   "source": [
    "## 4. Build datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-datasets",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset_for_match(match_id, compute_velocities=False):\n",
    "    \"\"\"Expand + create PassDatasetWithIdentity for one match.\"\"\"\n",
    "    em = build_expanded_dfs_with_identity(\n",
    "        all_combined[match_id],\n",
    "        all_lineups[match_id],\n",
    "        player_mapping,\n",
    "    )\n",
    "    ds = PassDatasetWithIdentity(\n",
    "        em.expanded_df,\n",
    "        only_passes=True,\n",
    "        compute_velocities=compute_velocities,\n",
    "    )\n",
    "    return ds, em\n",
    "\n",
    "\n",
    "# Build train datasets\n",
    "train_datasets = []\n",
    "for mid in train_ids:\n",
    "    ds, _ = build_dataset_for_match(mid)\n",
    "    train_datasets.append(ds)\n",
    "    print(f\"  train match {mid}: {len(ds)} pass samples\")\n",
    "\n",
    "train_ds = ConcatDataset(train_datasets)\n",
    "print(f\"Total training samples: {len(train_ds)}\")\n",
    "\n",
    "# Build val datasets\n",
    "val_datasets = []\n",
    "val_expanded = {}  # keep for visualization later\n",
    "for mid in val_ids:\n",
    "    ds, em = build_dataset_for_match(mid)\n",
    "    val_datasets.append(ds)\n",
    "    val_expanded[mid] = em\n",
    "    print(f\"  val match {mid}: {len(ds)} pass samples\")\n",
    "\n",
    "val_ds = ConcatDataset(val_datasets)\n",
    "print(f\"Total validation samples: {len(val_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "9ezmexkkp9i",
   "source": "# Free intermediate data no longer needed (save RAM)\nimport gc\n\ndel all_combined, all_events_df, all_lineups\ndel merged_events, merged_lineups\ngc.collect()\n\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n\nprint(\"Freed intermediate data.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "sec5-header",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hyperparams",
   "metadata": {},
   "outputs": [],
   "source": "# Hyperparameters\nEMBED_DIM = 8\nEPOCHS = 10\nBATCH_SIZE = 16\nLR = 1e-3\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nprint(f\"Device: {DEVICE}\")\nif DEVICE == \"cuda\":\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\nprint(f\"Embed dim: {EMBED_DIM}\")\nprint(f\"Epochs: {EPOCHS}, Batch size: {BATCH_SIZE}, LR: {LR}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collate-fn",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"Collate PassSampleWithIdentity into batched tensors.\"\"\"\n",
    "    channels = torch.stack([s.channels for s in batch])\n",
    "    actor_ids = torch.tensor([s.actor_id for s in batch], dtype=torch.long)\n",
    "    dest_indices = torch.tensor([s.dest_index for s in batch], dtype=torch.long)\n",
    "    completed = torch.tensor(\n",
    "        [s.completed if s.completed is not None else -1 for s in batch],\n",
    "        dtype=torch.float32,\n",
    "    )\n",
    "    return channels, actor_ids, dest_indices, completed\n",
    "\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "val_dl = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"Train batches: {len(train_dl)}, Val batches: {len(val_dl)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-init",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SoccerMapWithEmbedding(\n",
    "    num_players=num_players,\n",
    "    embed_dim=EMBED_DIM,\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "embed_params = sum(p.numel() for p in model.player_embedding.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Embedding parameters: {embed_params:,} ({100*embed_params/total_params:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-loop",
   "metadata": {},
   "outputs": [],
   "source": "train_losses = []\nval_losses = []\n\nfor epoch in range(EPOCHS):\n    # --- Train ---\n    model.train()\n    epoch_loss = 0.0\n    n_samples = 0\n\n    for channels, actor_ids, dest_indices, completed in train_dl:\n        channels = channels.to(DEVICE)\n        actor_ids = actor_ids.to(DEVICE)\n        dest_indices = dest_indices.to(DEVICE)\n\n        logits = model(channels, actor_ids)\n        loss = pass_selection_loss(logits, dest_indices)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss += loss.item() * len(channels)\n        n_samples += len(channels)\n\n    avg_train = epoch_loss / max(n_samples, 1)\n    train_losses.append(avg_train)\n\n    # --- Val ---\n    model.eval()\n    val_loss = 0.0\n    val_n = 0\n    with torch.no_grad():\n        for channels, actor_ids, dest_indices, completed in val_dl:\n            channels = channels.to(DEVICE)\n            actor_ids = actor_ids.to(DEVICE)\n            dest_indices = dest_indices.to(DEVICE)\n\n            logits = model(channels, actor_ids)\n            loss = pass_selection_loss(logits, dest_indices)\n\n            val_loss += loss.item() * len(channels)\n            val_n += len(channels)\n\n    avg_val = val_loss / max(val_n, 1)\n    val_losses.append(avg_val)\n\n    # GPU memory info\n    gpu_info = \"\"\n    if DEVICE == \"cuda\":\n        alloc = torch.cuda.memory_allocated() / 1024**2\n        reserved = torch.cuda.memory_reserved() / 1024**2\n        gpu_info = f\"  GPU: {alloc:.0f}/{reserved:.0f} MB (alloc/reserved)\"\n        torch.cuda.empty_cache()\n\n    print(f\"Epoch {epoch+1}/{EPOCHS}  train_loss={avg_train:.4f}  val_loss={avg_val:.4f}{gpu_info}\")\n\nprint(\"\\nTraining complete.\")"
  },
  {
   "cell_type": "markdown",
   "id": "sec6-header",
   "metadata": {},
   "source": [
    "## 6. Training curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loss-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.plot(range(1, EPOCHS + 1), train_losses, \"o-\", label=\"Train\")\n",
    "ax.plot(range(1, EPOCHS + 1), val_losses, \"s--\", label=\"Val\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Pass Selection Loss (CE)\")\n",
    "ax.set_title(\"Training Curve\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec7-header",
   "metadata": {},
   "source": [
    "## 7. Evaluation on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval-topk",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top-K accuracy: is the true destination in the top-K predicted pixels?\n",
    "model.eval()\n",
    "grid = GridSpec()\n",
    "\n",
    "top_k_values = [1, 5, 10, 50, 100]\n",
    "correct = {k: 0 for k in top_k_values}\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for channels, actor_ids, dest_indices, completed in val_dl:\n",
    "        channels = channels.to(DEVICE)\n",
    "        actor_ids = actor_ids.to(DEVICE)\n",
    "        logits = model(channels, actor_ids)\n",
    "\n",
    "        N = logits.shape[0]\n",
    "        flat = logits.view(N, -1)  # (N, L*W)\n",
    "\n",
    "        for k in top_k_values:\n",
    "            _, topk_idx = flat.topk(k, dim=1)\n",
    "            hits = (topk_idx == dest_indices.unsqueeze(1).to(DEVICE)).any(dim=1)\n",
    "            correct[k] += hits.sum().item()\n",
    "\n",
    "        total += N\n",
    "\n",
    "print(f\"Validation samples: {total}\")\n",
    "print(\"-\" * 35)\n",
    "for k in top_k_values:\n",
    "    acc = correct[k] / max(total, 1) * 100\n",
    "    print(f\"  Top-{k:>3d} accuracy: {acc:6.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec8-header",
   "metadata": {},
   "source": [
    "## 8. Player Similarity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similarity-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build reverse mapping: index -> name\n",
    "idx_to_name = {v: k for k, v in player_mapping.items()}\n",
    "\n",
    "# Get similarity matrix (excludes padding index 0)\n",
    "sim_matrix = model.get_similarity_matrix(exclude_padding=True)\n",
    "player_names = [idx_to_name[i] for i in range(1, num_players + 1)]\n",
    "\n",
    "print(f\"Similarity matrix shape: {sim_matrix.shape}\")\n",
    "print(f\"Players: {len(player_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sim-heatmap",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of player similarity\n",
    "# Use short names for readability\n",
    "short_names = [n.split()[-1] if len(n) > 15 else n for n in player_names]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 12))\n",
    "sns.heatmap(\n",
    "    sim_matrix,\n",
    "    xticklabels=short_names,\n",
    "    yticklabels=short_names,\n",
    "    cmap=\"RdBu_r\",\n",
    "    center=0,\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    square=True,\n",
    "    ax=ax,\n",
    ")\n",
    "ax.set_title(\"Player Embedding Cosine Similarity\")\n",
    "plt.xticks(rotation=90, fontsize=7)\n",
    "plt.yticks(rotation=0, fontsize=7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "top-pairs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top most-similar and most-different player pairs\n",
    "pairs = []\n",
    "for i in range(len(player_names)):\n",
    "    for j in range(i + 1, len(player_names)):\n",
    "        pairs.append((player_names[i], player_names[j], sim_matrix[i, j]))\n",
    "\n",
    "pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "print(\"Top 10 most SIMILAR player pairs:\")\n",
    "for a, b, s in pairs[:10]:\n",
    "    print(f\"  {s:+.4f}  {a}  <->  {b}\")\n",
    "\n",
    "print(\"\\nTop 10 most DIFFERENT player pairs:\")\n",
    "for a, b, s in pairs[-10:]:\n",
    "    print(f\"  {s:+.4f}  {a}  <->  {b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tsne",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "embeddings = model.get_player_embeddings()[1:]  # skip padding\n",
    "\n",
    "# Determine perplexity: must be < n_samples\n",
    "n = embeddings.shape[0]\n",
    "perp = min(30, max(2, n - 1))\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=perp, random_state=42)\n",
    "coords = tsne.fit_transform(embeddings)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.scatter(coords[:, 0], coords[:, 1], s=60, alpha=0.7)\n",
    "\n",
    "for i, name in enumerate(player_names):\n",
    "    short = name.split()[-1] if len(name) > 12 else name\n",
    "    ax.annotate(short, (coords[i, 0], coords[i, 1]),\n",
    "                fontsize=7, ha=\"center\", va=\"bottom\")\n",
    "\n",
    "ax.set_title(\"t-SNE of Player Embeddings\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec9-header",
   "metadata": {},
   "source": [
    "## 9. Save checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dir = PROJECT_ROOT / \"checkpoints\"\n",
    "ckpt_dir.mkdir(exist_ok=True)\n",
    "\n",
    "ckpt_path = ckpt_dir / \"player_embedding_pass_selection.pt\"\n",
    "torch.save({\n",
    "    \"model_state\": model.state_dict(),\n",
    "    \"player_mapping\": player_mapping,\n",
    "    \"num_players\": num_players,\n",
    "    \"embed_dim\": EMBED_DIM,\n",
    "    \"train_ids\": train_ids,\n",
    "    \"val_ids\": val_ids,\n",
    "    \"train_losses\": train_losses,\n",
    "    \"val_losses\": val_losses,\n",
    "}, ckpt_path)\n",
    "\n",
    "print(f\"Checkpoint saved: {ckpt_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec10-header",
   "metadata": {},
   "source": [
    "## 10. Visualize pass-selection surface for a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-sample",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a sample from val set\n",
    "sample = val_ds[0]\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    x = sample.channels.unsqueeze(0).to(DEVICE)\n",
    "    ids = torch.tensor([sample.actor_id], dtype=torch.long, device=DEVICE)\n",
    "    logits = model(x, ids)\n",
    "    surf = pass_selection_surface(logits)[0].cpu().numpy()  # (120, 80)\n",
    "\n",
    "player_name = idx_to_name.get(sample.actor_id, \"Unknown\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "im = ax.imshow(\n",
    "    surf.T,  # (W, L) for display\n",
    "    extent=[0, 120, 80, 0],\n",
    "    aspect=\"auto\",\n",
    "    cmap=\"hot\",\n",
    "    interpolation=\"bilinear\",\n",
    ")\n",
    "fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "\n",
    "# Mark true destination\n",
    "dl, dw = sample.dest_lw\n",
    "ax.scatter([dl], [dw], marker=\"x\", s=200, c=\"cyan\", linewidths=3, label=\"True dest\")\n",
    "\n",
    "ax.set_title(f\"Pass Selection Surface | {player_name} | event={sample.event_id[:8]}\")\n",
    "ax.set_xlabel(\"Pitch length\")\n",
    "ax.set_ylabel(\"Pitch width\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}